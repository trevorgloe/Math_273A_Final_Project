{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests of GD Convergence Depth\n",
    "\n",
    "This document is a test of the hidden-layer depth threshold for different monomial activation functions. Specifically, SGL and Manelli found that \n",
    "$$ k \\geq 2d $$\n",
    "is sufficient for ensuring GD converges to 0 and has no spurious local minima. We seek to find the relationship between $k$ and $d$ for other monomials\n",
    "\n",
    "Here we will use a single-layer monomial-activation neural network. In some tests, all the output weights are set to 1 to better match Manelli, but not always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inport nn files\n",
    "from experiment import *\n",
    "from monomial_neural_network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function that will make data and train a neural network using a given number of data points and epochs\n",
    "def test_training(n, k, M):\n",
    "    # n is the number of data points\n",
    "    # k is the hidden layer depth\n",
    "    # M is the number of epochs\n",
    "\n",
    "    d = 3 # just fix the dimension of the data for now\n",
    "    teacher_k = [k] # single layer\n",
    "    # teacher_model = generate_teacher_model_noOutWeight(d, teacher_k) # use unit weights for these calculations\n",
    "    teacher_model = generate_teacher_model(d, teacher_k)\n",
    "    print(teacher_model)\n",
    "\n",
    "    # generate data\n",
    "    data = generate_data(n, d, teacher_model)\n",
    "\n",
    "    # create student\n",
    "    student_k = [k] # student model hidden layer sizes - 2 layers with increasing number of neurons\n",
    "    # student_model = generate_student_model_noOutWeight(d, student_k)\n",
    "    student_model = generate_student_model(d, k=student_k)\n",
    "\n",
    "    # train the student\n",
    "    student_model, losses = train(\n",
    "        model = student_model, \n",
    "        x_train = data[0], \n",
    "        y_train= data[1], \n",
    "        num_epochs = M, \n",
    "        lr = 1e-3\n",
    "        )\n",
    "    \n",
    "    print(student_model.layers[0].weight)\n",
    "    print(teacher_model.layers[0].weight)\n",
    "    # return the final loss\n",
    "    return losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonomialNeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=4, bias=False)\n",
      "    (1): Monomial()\n",
      "    (2): Linear(in_features=4, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "starting training\n",
      "Epoch [0/10000], Loss: 160.25925\n",
      "Epoch [100/10000], Loss: 139.55611\n",
      "Epoch [200/10000], Loss: 122.23717\n",
      "Epoch [300/10000], Loss: 107.76143\n",
      "Epoch [400/10000], Loss: 95.65111\n",
      "Epoch [500/10000], Loss: 85.47652\n",
      "Epoch [600/10000], Loss: 76.83749\n",
      "Epoch [700/10000], Loss: 69.34380\n",
      "Epoch [800/10000], Loss: 62.60438\n",
      "Epoch [900/10000], Loss: 56.24401\n",
      "Epoch [1000/10000], Loss: 49.95488\n",
      "Epoch [1100/10000], Loss: 43.55092\n",
      "Epoch [1200/10000], Loss: 36.99002\n",
      "Epoch [1300/10000], Loss: 30.36780\n",
      "Epoch [1400/10000], Loss: 23.90591\n",
      "Epoch [1500/10000], Loss: 17.94465\n",
      "Epoch [1600/10000], Loss: 12.93723\n",
      "Epoch [1700/10000], Loss: 9.41938\n",
      "Epoch [1800/10000], Loss: 7.69846\n",
      "Epoch [1900/10000], Loss: 6.76842\n",
      "Epoch [2000/10000], Loss: 5.98887\n",
      "Epoch [2100/10000], Loss: 5.31767\n",
      "Epoch [2200/10000], Loss: 4.72771\n",
      "Epoch [2300/10000], Loss: 4.19504\n",
      "Epoch [2400/10000], Loss: 3.70172\n",
      "Epoch [2500/10000], Loss: 3.23784\n",
      "Epoch [2600/10000], Loss: 2.80136\n",
      "Epoch [2700/10000], Loss: 2.39648\n",
      "Epoch [2800/10000], Loss: 2.03173\n",
      "Epoch [2900/10000], Loss: 1.71821\n",
      "Epoch [3000/10000], Loss: 1.46753\n",
      "Epoch [3100/10000], Loss: 1.28750\n",
      "Epoch [3200/10000], Loss: 1.17248\n",
      "Epoch [3300/10000], Loss: 1.10742\n",
      "Epoch [3400/10000], Loss: 1.06317\n",
      "Epoch [3500/10000], Loss: 1.02960\n",
      "Epoch [3600/10000], Loss: 1.00328\n",
      "Epoch [3700/10000], Loss: 0.98225\n",
      "Epoch [3800/10000], Loss: 0.96521\n",
      "Epoch [3900/10000], Loss: 0.95123\n",
      "Epoch [4000/10000], Loss: 0.93965\n",
      "Epoch [4100/10000], Loss: 0.92998\n",
      "Epoch [4200/10000], Loss: 0.92184\n",
      "Epoch [4300/10000], Loss: 0.91495\n",
      "Epoch [4400/10000], Loss: 0.90909\n",
      "Epoch [4500/10000], Loss: 0.90407\n",
      "Epoch [4600/10000], Loss: 0.89976\n",
      "Epoch [4700/10000], Loss: 0.89604\n",
      "Epoch [4800/10000], Loss: 0.89283\n",
      "Epoch [4900/10000], Loss: 0.89003\n",
      "Epoch [5000/10000], Loss: 0.88760\n",
      "Epoch [5100/10000], Loss: 0.88547\n",
      "Epoch [5200/10000], Loss: 0.88361\n",
      "Epoch [5300/10000], Loss: 0.88197\n",
      "Epoch [5400/10000], Loss: 0.88053\n",
      "Epoch [5500/10000], Loss: 0.87926\n",
      "Epoch [5600/10000], Loss: 0.87813\n",
      "Epoch [5700/10000], Loss: 0.87713\n",
      "Epoch [5800/10000], Loss: 0.87625\n",
      "Epoch [5900/10000], Loss: 0.87546\n",
      "Epoch [6000/10000], Loss: 0.87475\n",
      "Epoch [6100/10000], Loss: 0.87412\n",
      "Epoch [6200/10000], Loss: 0.87356\n",
      "Epoch [6300/10000], Loss: 0.87305\n",
      "Epoch [6400/10000], Loss: 0.87259\n",
      "Epoch [6500/10000], Loss: 0.87218\n",
      "Epoch [6600/10000], Loss: 0.87181\n",
      "Epoch [6700/10000], Loss: 0.87147\n",
      "Epoch [6800/10000], Loss: 0.87117\n",
      "Epoch [6900/10000], Loss: 0.87089\n",
      "Epoch [7000/10000], Loss: 0.87064\n",
      "Epoch [7100/10000], Loss: 0.87041\n",
      "Epoch [7200/10000], Loss: 0.87020\n",
      "Epoch [7300/10000], Loss: 0.87001\n",
      "Epoch [7400/10000], Loss: 0.86983\n",
      "Epoch [7500/10000], Loss: 0.86967\n",
      "Epoch [7600/10000], Loss: 0.86952\n",
      "Epoch [7700/10000], Loss: 0.86939\n",
      "Epoch [7800/10000], Loss: 0.86926\n",
      "Epoch [7900/10000], Loss: 0.86915\n",
      "Epoch [8000/10000], Loss: 0.86904\n",
      "Epoch [8100/10000], Loss: 0.86894\n",
      "Epoch [8200/10000], Loss: 0.86885\n",
      "Epoch [8300/10000], Loss: 0.86876\n",
      "Epoch [8400/10000], Loss: 0.86868\n",
      "Epoch [8500/10000], Loss: 0.86861\n",
      "Epoch [8600/10000], Loss: 0.86854\n",
      "Epoch [8700/10000], Loss: 0.86847\n",
      "Epoch [8800/10000], Loss: 0.86841\n",
      "Epoch [8900/10000], Loss: 0.86836\n",
      "Epoch [9000/10000], Loss: 0.86830\n",
      "Epoch [9100/10000], Loss: 0.86825\n",
      "Epoch [9200/10000], Loss: 0.86821\n",
      "Epoch [9300/10000], Loss: 0.86816\n",
      "Epoch [9400/10000], Loss: 0.86812\n",
      "Epoch [9500/10000], Loss: 0.86808\n",
      "Epoch [9600/10000], Loss: 0.86805\n",
      "Epoch [9700/10000], Loss: 0.86801\n",
      "Epoch [9800/10000], Loss: 0.86798\n",
      "Epoch [9900/10000], Loss: 0.86795\n",
      "\n",
      "Training Complete\n",
      "Parameter containing:\n",
      "tensor([[ 8.7876e-02,  2.8508e-01, -1.8265e-02],\n",
      "        [ 1.6108e-01,  1.6165e+00,  3.5469e-03],\n",
      "        [ 4.1301e-02,  6.0607e-01,  3.3349e-03],\n",
      "        [-3.1448e-02, -3.1947e-01,  3.5500e-04]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1183,  0.9309, -1.0015],\n",
      "        [ 0.6932,  0.5296,  0.5770],\n",
      "        [-0.5516,  1.0479, -0.1747],\n",
      "        [-0.0592,  0.6069,  0.9528]], requires_grad=True)\n",
      "0.8679218292236328\n"
     ]
    }
   ],
   "source": [
    "# test out the function\n",
    "l = test_training(n=10, k=4, M=10000)\n",
    "print(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

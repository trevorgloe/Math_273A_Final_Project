{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests of GD Convergence Depth\n",
    "\n",
    "This document is a test of the hidden-layer depth threshold for different monomial activation functions. Specifically, SGL and Manelli found that \n",
    "$$ k \\geq 2d $$\n",
    "is sufficient for ensuring GD converges to 0 and has no spurious local minima. We seek to find the relationship between $k$ and $d$ for other monomials\n",
    "\n",
    "Here we will use a single-layer monomial-activation neural network with output weights all set to be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inport nn files\n",
    "from experiment import *\n",
    "from monomial_neural_network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function that will make data and train a neural network using a given number of data points and epochs\n",
    "def test_training(n, k, M):\n",
    "    # n is the number of data points\n",
    "    # k is the hidden layer depth\n",
    "    # M is the number of epochs\n",
    "\n",
    "    d = 3 # just fix the dimension of the data for now\n",
    "    teacher_k = [k] # single layer\n",
    "    teacher_model = generate_teacher_model_noOutWeight(d, teacher_k) # use unit weights for these calculations\n",
    "    print(teacher_model)\n",
    "\n",
    "    # generate data\n",
    "    data = generate_data(n, d, teacher_model)\n",
    "\n",
    "    # create student\n",
    "    student_k = [k] # student model hidden layer sizes - 2 layers with increasing number of neurons\n",
    "    student_model = generate_student_model_noOutWeight(d, student_k)\n",
    "\n",
    "    # train the student\n",
    "    student_model, losses = train(\n",
    "        model = student_model, \n",
    "        x_train = data[0], \n",
    "        y_train= data[1], \n",
    "        num_epochs = M, \n",
    "        lr = 1e-3\n",
    "        )\n",
    "    \n",
    "    print(student_model.layers[0].weight)\n",
    "    print(teacher_model.layers[0].weight)\n",
    "    # return the final loss\n",
    "    return losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonomialNeuralNetwork_noOutputWeight(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=False)\n",
      "    (1): Monomial()\n",
      "    (2): Linear(in_features=2, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "starting training\n",
      "Epoch [0/10000], Loss: 17.46714\n",
      "Epoch [100/10000], Loss: 16.61201\n",
      "Epoch [200/10000], Loss: 15.54452\n",
      "Epoch [300/10000], Loss: 14.28187\n",
      "Epoch [400/10000], Loss: 12.85036\n",
      "Epoch [500/10000], Loss: 11.28392\n",
      "Epoch [600/10000], Loss: 9.62347\n",
      "Epoch [700/10000], Loss: 7.91661\n",
      "Epoch [800/10000], Loss: 6.21747\n",
      "Epoch [900/10000], Loss: 4.58663\n",
      "Epoch [1000/10000], Loss: 3.09107\n",
      "Epoch [1100/10000], Loss: 1.80410\n",
      "Epoch [1200/10000], Loss: 0.80525\n",
      "Epoch [1300/10000], Loss: 0.17995\n",
      "Epoch [1400/10000], Loss: 0.00503\n",
      "Epoch [1500/10000], Loss: 0.00160\n",
      "Epoch [1600/10000], Loss: 0.00078\n",
      "Epoch [1700/10000], Loss: 0.00047\n",
      "Epoch [1800/10000], Loss: 0.00031\n",
      "Epoch [1900/10000], Loss: 0.00021\n",
      "Epoch [2000/10000], Loss: 0.00014\n",
      "Epoch [2100/10000], Loss: 0.00010\n",
      "Epoch [2200/10000], Loss: 0.00006\n",
      "Epoch [2300/10000], Loss: 0.00004\n",
      "Epoch [2400/10000], Loss: 0.00003\n",
      "Epoch [2500/10000], Loss: 0.00002\n",
      "Epoch [2600/10000], Loss: 0.00001\n",
      "Epoch [2700/10000], Loss: 0.00001\n",
      "Epoch [2800/10000], Loss: 0.00001\n",
      "Epoch [2900/10000], Loss: 0.00000\n",
      "Epoch [3000/10000], Loss: 0.00000\n",
      "Epoch [3100/10000], Loss: 0.00000\n",
      "Epoch [3200/10000], Loss: 0.00000\n",
      "Epoch [3300/10000], Loss: 0.00000\n",
      "Epoch [3400/10000], Loss: 0.00000\n",
      "Epoch [3500/10000], Loss: 0.00000\n",
      "Epoch [3600/10000], Loss: 0.00000\n",
      "Epoch [3700/10000], Loss: 0.00000\n",
      "Epoch [3800/10000], Loss: 0.00000\n",
      "Epoch [3900/10000], Loss: 0.00000\n",
      "Epoch [4000/10000], Loss: 0.00000\n",
      "Epoch [4100/10000], Loss: 0.00000\n",
      "Epoch [4200/10000], Loss: 0.00000\n",
      "Epoch [4300/10000], Loss: 0.00000\n",
      "Epoch [4400/10000], Loss: 0.00000\n",
      "Epoch [4500/10000], Loss: 0.00000\n",
      "Epoch [4600/10000], Loss: 0.00000\n",
      "Epoch [4700/10000], Loss: 0.00000\n",
      "Epoch [4800/10000], Loss: 0.00000\n",
      "Epoch [4900/10000], Loss: 0.00000\n",
      "Epoch [5000/10000], Loss: 0.00000\n",
      "Epoch [5100/10000], Loss: 0.00000\n",
      "Epoch [5200/10000], Loss: 0.00000\n",
      "Epoch [5300/10000], Loss: 0.00000\n",
      "Epoch [5400/10000], Loss: 0.00000\n",
      "Epoch [5500/10000], Loss: 0.00000\n",
      "Epoch [5600/10000], Loss: 0.00000\n",
      "Epoch [5700/10000], Loss: 0.00000\n",
      "Epoch [5800/10000], Loss: 0.00000\n",
      "Epoch [5900/10000], Loss: 0.00000\n",
      "Epoch [6000/10000], Loss: 0.00000\n",
      "Epoch [6100/10000], Loss: 0.00000\n",
      "Epoch [6200/10000], Loss: 0.00000\n",
      "Epoch [6300/10000], Loss: 0.00000\n",
      "Epoch [6400/10000], Loss: 0.00000\n",
      "Epoch [6500/10000], Loss: 0.00000\n",
      "Epoch [6600/10000], Loss: 0.00000\n",
      "Epoch [6700/10000], Loss: 0.00000\n",
      "Epoch [6800/10000], Loss: 0.00000\n",
      "Epoch [6900/10000], Loss: 0.00000\n",
      "Epoch [7000/10000], Loss: 0.00000\n",
      "Epoch [7100/10000], Loss: 0.00000\n",
      "Epoch [7200/10000], Loss: 0.00000\n",
      "Epoch [7300/10000], Loss: 0.00000\n",
      "Epoch [7400/10000], Loss: 0.00000\n",
      "Epoch [7500/10000], Loss: 0.00000\n",
      "Epoch [7600/10000], Loss: 0.00000\n",
      "Epoch [7700/10000], Loss: 0.00000\n",
      "Epoch [7800/10000], Loss: 0.00000\n",
      "Epoch [7900/10000], Loss: 0.00000\n",
      "Epoch [8000/10000], Loss: 0.00000\n",
      "Epoch [8100/10000], Loss: 0.00000\n",
      "Epoch [8200/10000], Loss: 0.00000\n",
      "Epoch [8300/10000], Loss: 0.00000\n",
      "Epoch [8400/10000], Loss: 0.00000\n",
      "Epoch [8500/10000], Loss: 0.00000\n",
      "Epoch [8600/10000], Loss: 0.00000\n",
      "Epoch [8700/10000], Loss: 0.00000\n",
      "Epoch [8800/10000], Loss: 0.00000\n",
      "Epoch [8900/10000], Loss: 0.00000\n",
      "Epoch [9000/10000], Loss: 0.00000\n",
      "Epoch [9100/10000], Loss: 0.00000\n",
      "Epoch [9200/10000], Loss: 0.00000\n",
      "Epoch [9300/10000], Loss: 0.00000\n",
      "Epoch [9400/10000], Loss: 0.00000\n",
      "Epoch [9500/10000], Loss: 0.00000\n",
      "Epoch [9600/10000], Loss: 0.00000\n",
      "Epoch [9700/10000], Loss: 0.00000\n",
      "Epoch [9800/10000], Loss: 0.00000\n",
      "Epoch [9900/10000], Loss: 0.00000\n",
      "\n",
      "Training Complete\n",
      "Parameter containing:\n",
      "tensor([[-0.8198,  1.2466],\n",
      "        [-1.2008, -0.0256]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.3539,  0.2543],\n",
      "        [-0.5300,  1.2207]], requires_grad=True)\n",
      "6.078497705175323e-10\n"
     ]
    }
   ],
   "source": [
    "# test out the function\n",
    "l = test_training(n=10, k=4, M=10000)\n",
    "print(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

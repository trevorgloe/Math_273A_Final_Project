{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monomial_neural_network import *\n",
    "from experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonomialNeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=3, bias=False)\n",
      "    (1): Monomial()\n",
      "    (2): Linear(in_features=3, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "tensor([-0.6756,  0.1704]) tensor([1.5375]) tensor([1.5375])\n",
      "MonomialNeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=3, bias=False)\n",
      "    (1): Monomial()\n",
      "    (2): Linear(in_features=3, out_features=1, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "d = 2 # input data dimension\n",
    "teacher_k = [4] # teacher model hidden layer sizes - 3 layers with increasing number of neurons\n",
    "# teacher_k = [10] # teacher model hidden layer sizes - 1 layer with 10 neurons\n",
    "\n",
    "teacher_model = generate_teacher_model(d, teacher_k)\n",
    "print(teacher_model)\n",
    "\n",
    "n = 1000 # number of data points\n",
    "# same data dimension d as before\n",
    "\n",
    "data = generate_data(n, d, teacher_model)\n",
    "\n",
    "# verify that the data is generated correctly\n",
    "print(data[0][0], data[1][0], teacher_model.evaluate(data[0][0]))\n",
    "\n",
    "student_k = [4] # student model hidden layer sizes - 2 layers with increasing number of neurons\n",
    "student_model = generate_student_model(d, student_k)\n",
    "print(student_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Epoch [0/20000], Loss: 42.20013\n",
      "Epoch [100/20000], Loss: 37.63603\n",
      "Epoch [200/20000], Loss: 33.15986\n",
      "Epoch [300/20000], Loss: 28.69736\n",
      "Epoch [400/20000], Loss: 24.23464\n",
      "Epoch [500/20000], Loss: 19.81615\n",
      "Epoch [600/20000], Loss: 15.54110\n",
      "Epoch [700/20000], Loss: 11.56037\n",
      "Epoch [800/20000], Loss: 8.07281\n",
      "Epoch [900/20000], Loss: 5.31356\n",
      "Epoch [1000/20000], Loss: 3.50147\n",
      "Epoch [1100/20000], Loss: 2.57107\n",
      "Epoch [1200/20000], Loss: 2.01316\n",
      "Epoch [1300/20000], Loss: 1.63393\n",
      "Epoch [1400/20000], Loss: 1.38510\n",
      "Epoch [1500/20000], Loss: 1.22775\n",
      "Epoch [1600/20000], Loss: 1.14757\n",
      "Epoch [1700/20000], Loss: 1.12256\n",
      "Epoch [1800/20000], Loss: 1.11463\n",
      "Epoch [1900/20000], Loss: 1.11207\n",
      "Epoch [2000/20000], Loss: 1.11124\n",
      "Epoch [2100/20000], Loss: 1.11097\n",
      "Epoch [2200/20000], Loss: 1.11088\n",
      "Epoch [2300/20000], Loss: 1.11085\n",
      "Epoch [2400/20000], Loss: 1.11084\n",
      "Epoch [2500/20000], Loss: 1.11083\n",
      "Epoch [2600/20000], Loss: 1.11083\n",
      "Epoch [2700/20000], Loss: 1.11083\n",
      "Epoch [2800/20000], Loss: 1.11083\n",
      "Epoch [2900/20000], Loss: 1.11083\n",
      "Epoch [3000/20000], Loss: 1.11083\n",
      "Epoch [3100/20000], Loss: 1.11083\n",
      "Epoch [3200/20000], Loss: 1.11083\n",
      "Epoch [3300/20000], Loss: 1.11083\n",
      "Epoch [3400/20000], Loss: 1.11083\n",
      "Epoch [3500/20000], Loss: 1.11083\n",
      "Epoch [3600/20000], Loss: 1.11083\n",
      "Epoch [3700/20000], Loss: 1.11083\n",
      "Epoch [3800/20000], Loss: 1.11083\n",
      "Epoch [3900/20000], Loss: 1.11083\n",
      "Epoch [4000/20000], Loss: 1.11083\n",
      "Epoch [4100/20000], Loss: 1.11083\n",
      "Epoch [4200/20000], Loss: 1.11083\n",
      "Epoch [4300/20000], Loss: 1.11083\n",
      "Epoch [4400/20000], Loss: 1.11083\n",
      "Epoch [4500/20000], Loss: 1.11083\n",
      "Epoch [4600/20000], Loss: 1.11083\n",
      "Epoch [4700/20000], Loss: 1.11083\n",
      "Epoch [4800/20000], Loss: 1.11083\n",
      "Epoch [4900/20000], Loss: 1.11083\n",
      "Epoch [5000/20000], Loss: 1.11083\n",
      "Epoch [5100/20000], Loss: 1.11083\n",
      "Epoch [5200/20000], Loss: 1.11083\n",
      "Epoch [5300/20000], Loss: 1.11083\n",
      "Epoch [5400/20000], Loss: 1.11083\n",
      "Epoch [5500/20000], Loss: 1.11083\n",
      "Epoch [5600/20000], Loss: 1.11083\n",
      "Epoch [5700/20000], Loss: 1.11083\n",
      "Epoch [5800/20000], Loss: 1.11083\n",
      "Epoch [5900/20000], Loss: 1.11083\n",
      "Epoch [6000/20000], Loss: 1.11083\n",
      "Epoch [6100/20000], Loss: 1.11083\n",
      "Epoch [6200/20000], Loss: 1.11083\n",
      "Epoch [6300/20000], Loss: 1.11083\n",
      "Epoch [6400/20000], Loss: 1.11083\n",
      "Epoch [6500/20000], Loss: 1.11083\n",
      "Epoch [6600/20000], Loss: 1.11083\n",
      "Epoch [6700/20000], Loss: 1.11083\n",
      "Epoch [6800/20000], Loss: 1.11083\n",
      "Epoch [6900/20000], Loss: 1.11083\n",
      "Epoch [7000/20000], Loss: 1.11083\n",
      "Epoch [7100/20000], Loss: 1.11083\n",
      "Epoch [7200/20000], Loss: 1.11083\n",
      "Epoch [7300/20000], Loss: 1.11083\n",
      "Epoch [7400/20000], Loss: 1.11083\n",
      "Epoch [7500/20000], Loss: 1.11083\n",
      "Epoch [7600/20000], Loss: 1.11083\n",
      "Epoch [7700/20000], Loss: 1.11083\n",
      "Epoch [7800/20000], Loss: 1.11083\n",
      "Epoch [7900/20000], Loss: 1.11083\n",
      "Epoch [8000/20000], Loss: 1.11083\n",
      "Epoch [8100/20000], Loss: 1.11083\n",
      "Epoch [8200/20000], Loss: 1.11083\n",
      "Epoch [8300/20000], Loss: 1.11083\n",
      "Epoch [8400/20000], Loss: 1.11083\n",
      "Epoch [8500/20000], Loss: 1.11083\n",
      "Epoch [8600/20000], Loss: 1.11083\n",
      "Epoch [8700/20000], Loss: 1.11083\n",
      "Epoch [8800/20000], Loss: 1.11083\n",
      "Epoch [8900/20000], Loss: 1.11083\n",
      "Epoch [9000/20000], Loss: 1.11083\n",
      "Epoch [9100/20000], Loss: 1.11083\n",
      "Epoch [9200/20000], Loss: 1.11083\n",
      "Epoch [9300/20000], Loss: 1.11083\n",
      "Epoch [9400/20000], Loss: 1.11083\n",
      "Epoch [9500/20000], Loss: 1.11083\n",
      "Epoch [9600/20000], Loss: 1.11083\n",
      "Epoch [9700/20000], Loss: 1.11083\n",
      "Epoch [9800/20000], Loss: 1.11083\n",
      "Epoch [9900/20000], Loss: 1.11083\n",
      "Epoch [10000/20000], Loss: 1.11083\n",
      "Epoch [10100/20000], Loss: 1.11083\n",
      "Epoch [10200/20000], Loss: 1.11083\n",
      "Epoch [10300/20000], Loss: 1.11083\n",
      "Epoch [10400/20000], Loss: 1.11083\n",
      "Epoch [10500/20000], Loss: 1.11083\n",
      "Epoch [10600/20000], Loss: 1.11083\n",
      "Epoch [10700/20000], Loss: 1.11083\n",
      "Epoch [10800/20000], Loss: 1.11083\n",
      "Epoch [10900/20000], Loss: 1.11083\n",
      "Epoch [11000/20000], Loss: 1.11083\n",
      "Epoch [11100/20000], Loss: 1.11083\n",
      "Epoch [11200/20000], Loss: 1.11083\n",
      "Epoch [11300/20000], Loss: 1.11083\n",
      "Epoch [11400/20000], Loss: 1.11083\n",
      "Epoch [11500/20000], Loss: 1.11083\n",
      "Epoch [11600/20000], Loss: 1.11083\n",
      "Epoch [11700/20000], Loss: 1.11083\n",
      "Epoch [11800/20000], Loss: 1.11083\n",
      "Epoch [11900/20000], Loss: 1.11083\n",
      "Epoch [12000/20000], Loss: 1.11083\n",
      "Epoch [12100/20000], Loss: 1.11083\n",
      "Epoch [12200/20000], Loss: 1.11083\n",
      "Epoch [12300/20000], Loss: 1.11083\n",
      "Epoch [12400/20000], Loss: 1.11083\n",
      "Epoch [12500/20000], Loss: 1.11083\n",
      "Epoch [12600/20000], Loss: 1.11083\n",
      "Epoch [12700/20000], Loss: 1.11083\n",
      "Epoch [12800/20000], Loss: 1.11083\n",
      "Epoch [12900/20000], Loss: 1.11083\n",
      "Epoch [13000/20000], Loss: 1.11083\n",
      "Epoch [13100/20000], Loss: 1.11083\n",
      "Epoch [13200/20000], Loss: 1.11083\n",
      "Epoch [13300/20000], Loss: 1.11083\n",
      "Epoch [13400/20000], Loss: 1.11083\n",
      "Epoch [13500/20000], Loss: 1.11083\n",
      "Epoch [13600/20000], Loss: 1.11083\n",
      "Epoch [13700/20000], Loss: 1.11083\n",
      "Epoch [13800/20000], Loss: 1.11083\n",
      "Epoch [13900/20000], Loss: 1.11083\n",
      "Epoch [14000/20000], Loss: 1.11083\n",
      "Epoch [14100/20000], Loss: 1.11083\n",
      "Epoch [14200/20000], Loss: 1.11083\n",
      "Epoch [14300/20000], Loss: 1.11083\n",
      "Epoch [14400/20000], Loss: 1.11083\n",
      "Epoch [14500/20000], Loss: 1.11083\n",
      "Epoch [14600/20000], Loss: 1.11083\n",
      "Epoch [14700/20000], Loss: 1.11083\n",
      "Epoch [14800/20000], Loss: 1.11083\n",
      "Epoch [14900/20000], Loss: 1.11083\n",
      "Epoch [15000/20000], Loss: 1.11083\n",
      "Epoch [15100/20000], Loss: 1.11083\n",
      "Epoch [15200/20000], Loss: 1.11083\n",
      "Epoch [15300/20000], Loss: 1.11083\n",
      "Epoch [15400/20000], Loss: 1.11083\n",
      "Epoch [15500/20000], Loss: 1.11083\n",
      "Epoch [15600/20000], Loss: 1.11083\n",
      "Epoch [15700/20000], Loss: 1.11083\n",
      "Epoch [15800/20000], Loss: 1.11083\n",
      "Epoch [15900/20000], Loss: 1.11083\n",
      "Epoch [16000/20000], Loss: 1.11083\n",
      "Epoch [16100/20000], Loss: 1.11083\n",
      "Epoch [16200/20000], Loss: 1.11083\n",
      "Epoch [16300/20000], Loss: 1.11083\n",
      "Epoch [16400/20000], Loss: 1.11083\n",
      "Epoch [16500/20000], Loss: 1.11083\n",
      "Epoch [16600/20000], Loss: 1.11083\n",
      "Epoch [16700/20000], Loss: 1.11083\n",
      "Epoch [16800/20000], Loss: 1.11083\n",
      "Epoch [16900/20000], Loss: 1.11083\n",
      "Epoch [17000/20000], Loss: 1.11083\n",
      "Epoch [17100/20000], Loss: 1.11083\n",
      "Epoch [17200/20000], Loss: 1.11083\n",
      "Epoch [17300/20000], Loss: 1.11083\n",
      "Epoch [17400/20000], Loss: 1.11083\n",
      "Epoch [17500/20000], Loss: 1.11083\n",
      "Epoch [17600/20000], Loss: 1.11083\n",
      "Epoch [17700/20000], Loss: 1.11083\n",
      "Epoch [17800/20000], Loss: 1.11083\n",
      "Epoch [17900/20000], Loss: 1.11083\n",
      "Epoch [18000/20000], Loss: 1.11083\n",
      "Epoch [18100/20000], Loss: 1.11083\n",
      "Epoch [18200/20000], Loss: 1.11083\n",
      "Epoch [18300/20000], Loss: 1.11083\n",
      "Epoch [18400/20000], Loss: 1.11083\n",
      "Epoch [18500/20000], Loss: 1.11083\n",
      "Epoch [18600/20000], Loss: 1.11083\n",
      "Epoch [18700/20000], Loss: 1.11083\n",
      "Epoch [18800/20000], Loss: 1.11083\n",
      "Epoch [18900/20000], Loss: 1.11083\n",
      "Epoch [19000/20000], Loss: 1.11083\n",
      "Epoch [19100/20000], Loss: 1.11083\n",
      "Epoch [19200/20000], Loss: 1.11083\n",
      "Epoch [19300/20000], Loss: 1.11083\n",
      "Epoch [19400/20000], Loss: 1.11083\n",
      "Epoch [19500/20000], Loss: 1.11083\n",
      "Epoch [19600/20000], Loss: 1.11083\n",
      "Epoch [19700/20000], Loss: 1.11083\n",
      "Epoch [19800/20000], Loss: 1.11083\n",
      "Epoch [19900/20000], Loss: 1.11083\n",
      "\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# train the student\n",
    "student_model, losses = train(\n",
    "    model = student_model, \n",
    "    x_train = data[0], \n",
    "    y_train= data[1], \n",
    "    num_epochs = 20000, \n",
    "    lr = 1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.938348  -1.1528543]\n",
      " [-1.1528543  0.4523199]]\n",
      "[[ 2.8567715  -0.89306384]\n",
      " [-0.89306384  0.96261173]]\n",
      "[3.390668e+00 5.272745e-08]\n",
      "[3.21143  0.607953]\n",
      "0.0009902802124023438\n"
     ]
    }
   ],
   "source": [
    "# lets look at the weight matrices\n",
    "# print(student_model.layers[0].weight)\n",
    "student_w = student_model.layers[0].weight.detach().numpy()\n",
    "student_v = student_model.layers[2].weight.detach().numpy()\n",
    "# print(student_w)\n",
    "teacher_w = teacher_model.layers[0].weight.detach().numpy()\n",
    "teacher_v = teacher_model.layers[2].weight.detach().numpy()\n",
    "# print(student_w)\n",
    "# print(teacher_w)\n",
    "# print(np.squeeze(student_v))\n",
    "A = student_w.T @ np.diag(np.squeeze(student_v)) @ student_w\n",
    "Astar = teacher_w.T @ np.diag(np.squeeze(teacher_v)) @ teacher_w\n",
    "print(A)\n",
    "print(Astar)\n",
    "eigA, eigvecA = np.linalg.eig(A)\n",
    "print(eigA)\n",
    "# print(Astar)\n",
    "eigAstar, eigvecAstar = np.linalg.eig(Astar)\n",
    "print(eigAstar)\n",
    "e = pop_loss(student_model, teacher_model, d=d, N=1000000)\n",
    "# print(y_teach)\n",
    "# print(y_stud)\n",
    "# print(torch.norm(y_teach))\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.01623166]]\n",
      "tensor([[1., 2.]])\n",
      "tensor([[11.4299]], grad_fn=<MmBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.4712,  1.2445,  1.0397, -1.0011]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_vec = np.array([[1,2]]).T\n",
    "print(test_vec.T@A@test_vec)\n",
    "# print(test_vec.T@Astar@test_vec)\n",
    "test_vec_torch = torch.from_numpy(test_vec.T).type(torch.FloatTensor)\n",
    "print(test_vec_torch)\n",
    "print(student_model(test_vec_torch))\n",
    "print(student_model.layers[2].weight)\n",
    "print(teacher_model.layers[2].weight)\n",
    "# print(teacher_model(test_vec_torch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

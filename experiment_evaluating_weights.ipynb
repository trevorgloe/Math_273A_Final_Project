{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monomial_neural_network import *\n",
    "from experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonomialNeuralNetwork_noOutputWeight(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=10, bias=False)\n",
      "    (1): Monomial()\n",
      "    (2): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "(tensor([[ 0.1857,  1.9637,  2.1015, -0.7523],\n",
      "        [ 1.4161,  1.1846, -0.5858,  0.1405],\n",
      "        [ 0.9466,  0.0789,  0.2166,  0.4672],\n",
      "        [ 0.7652,  1.1856, -0.4130, -1.6143],\n",
      "        [-1.8120,  1.7171, -1.1413,  1.4184],\n",
      "        [-1.1497, -2.0946, -0.6056,  2.8334],\n",
      "        [ 0.4669,  0.6362, -1.2938,  0.9946],\n",
      "        [-0.3734, -0.5143, -0.8752, -2.6766],\n",
      "        [-1.5875,  1.1046, -0.4019,  2.1080],\n",
      "        [ 1.7395,  0.2512,  0.5841,  0.7440]]), tensor([[10.1249],\n",
      "        [ 7.9558],\n",
      "        [ 2.5299],\n",
      "        [ 7.6739],\n",
      "        [36.3577],\n",
      "        [13.0504],\n",
      "        [ 6.0467],\n",
      "        [14.9172],\n",
      "        [29.4708],\n",
      "        [ 8.6245]]))\n",
      "tensor([ 0.1857,  1.9637,  2.1015, -0.7523]) tensor([10.1249]) tensor([10.1249])\n",
      "MonomialNeuralNetwork_noOutputWeight(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=10, bias=False)\n",
      "    (1): Monomial()\n",
      "    (2): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "d = 4 # input data dimension\n",
    "teacher_k = [10] # teacher model hidden layer sizes - 3 layers with increasing number of neurons\n",
    "# teacher_k = [10] # teacher model hidden layer sizes - 1 layer with 10 neurons\n",
    "\n",
    "teacher_model = generate_teacher_model_noOutWeight(d, teacher_k)\n",
    "print(teacher_model)\n",
    "\n",
    "n = 11 # number of data points\n",
    "# same data dimension d as before\n",
    "\n",
    "data = generate_data(n, d, teacher_model)\n",
    "\n",
    "# verify that the data is generated correctly\n",
    "print(data)\n",
    "print(data[0][0], data[1][0], teacher_model.evaluate(data[0][0]))\n",
    "\n",
    "student_k = [10] # student model hidden layer sizes - 2 layers with increasing number of neurons\n",
    "student_model = generate_student_model_noOutWeight(d, student_k)\n",
    "print(student_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Epoch [0/40000], Loss: 110.37209\n",
      "Epoch [100/40000], Loss: 85.65590\n",
      "Epoch [200/40000], Loss: 65.41264\n",
      "Epoch [300/40000], Loss: 48.85165\n",
      "Epoch [400/40000], Loss: 35.36958\n",
      "Epoch [500/40000], Loss: 24.55598\n",
      "Epoch [600/40000], Loss: 16.15194\n",
      "Epoch [700/40000], Loss: 9.96635\n",
      "Epoch [800/40000], Loss: 5.76595\n",
      "Epoch [900/40000], Loss: 3.17901\n",
      "Epoch [1000/40000], Loss: 1.70527\n",
      "Epoch [1100/40000], Loss: 0.88796\n",
      "Epoch [1200/40000], Loss: 0.45385\n",
      "Epoch [1300/40000], Loss: 0.23012\n",
      "Epoch [1400/40000], Loss: 0.10620\n",
      "Epoch [1500/40000], Loss: 0.05237\n",
      "Epoch [1600/40000], Loss: 0.03095\n",
      "Epoch [1700/40000], Loss: 0.02072\n",
      "Epoch [1800/40000], Loss: 0.01489\n",
      "Epoch [1900/40000], Loss: 0.01111\n",
      "Epoch [2000/40000], Loss: 0.00846\n",
      "Epoch [2100/40000], Loss: 0.00653\n",
      "Epoch [2200/40000], Loss: 0.00508\n",
      "Epoch [2300/40000], Loss: 0.00400\n",
      "Epoch [2400/40000], Loss: 0.00318\n",
      "Epoch [2500/40000], Loss: 0.00257\n",
      "Epoch [2600/40000], Loss: 0.00210\n",
      "Epoch [2700/40000], Loss: 0.00174\n",
      "Epoch [2800/40000], Loss: 0.00147\n",
      "Epoch [2900/40000], Loss: 0.00126\n",
      "Epoch [3000/40000], Loss: 0.00110\n",
      "Epoch [3100/40000], Loss: 0.00098\n",
      "Epoch [3200/40000], Loss: 0.00088\n",
      "Epoch [3300/40000], Loss: 0.00081\n",
      "Epoch [3400/40000], Loss: 0.00075\n",
      "Epoch [3500/40000], Loss: 0.00071\n",
      "Epoch [3600/40000], Loss: 0.00067\n",
      "Epoch [3700/40000], Loss: 0.00065\n",
      "Epoch [3800/40000], Loss: 0.00063\n",
      "Epoch [3900/40000], Loss: 0.00061\n",
      "Epoch [4000/40000], Loss: 0.00059\n",
      "Epoch [4100/40000], Loss: 0.00058\n",
      "Epoch [4200/40000], Loss: 0.00057\n",
      "Epoch [4300/40000], Loss: 0.00056\n",
      "Epoch [4400/40000], Loss: 0.00055\n",
      "Epoch [4500/40000], Loss: 0.00054\n",
      "Epoch [4600/40000], Loss: 0.00054\n",
      "Epoch [4700/40000], Loss: 0.00053\n",
      "Epoch [4800/40000], Loss: 0.00053\n",
      "Epoch [4900/40000], Loss: 0.00052\n",
      "Epoch [5000/40000], Loss: 0.00052\n",
      "Epoch [5100/40000], Loss: 0.00051\n",
      "Epoch [5200/40000], Loss: 0.00051\n",
      "Epoch [5300/40000], Loss: 0.00050\n",
      "Epoch [5400/40000], Loss: 0.00050\n",
      "Epoch [5500/40000], Loss: 0.00049\n",
      "Epoch [5600/40000], Loss: 0.00049\n",
      "Epoch [5700/40000], Loss: 0.00048\n",
      "Epoch [5800/40000], Loss: 0.00048\n",
      "Epoch [5900/40000], Loss: 0.00048\n",
      "Epoch [6000/40000], Loss: 0.00047\n",
      "Epoch [6100/40000], Loss: 0.00047\n",
      "Epoch [6200/40000], Loss: 0.00046\n",
      "Epoch [6300/40000], Loss: 0.00046\n",
      "Epoch [6400/40000], Loss: 0.00046\n",
      "Epoch [6500/40000], Loss: 0.00045\n",
      "Epoch [6600/40000], Loss: 0.00045\n",
      "Epoch [6700/40000], Loss: 0.00044\n",
      "Epoch [6800/40000], Loss: 0.00044\n",
      "Epoch [6900/40000], Loss: 0.00044\n",
      "Epoch [7000/40000], Loss: 0.00043\n",
      "Epoch [7100/40000], Loss: 0.00043\n",
      "Epoch [7200/40000], Loss: 0.00043\n",
      "Epoch [7300/40000], Loss: 0.00042\n",
      "Epoch [7400/40000], Loss: 0.00042\n",
      "Epoch [7500/40000], Loss: 0.00042\n",
      "Epoch [7600/40000], Loss: 0.00041\n",
      "Epoch [7700/40000], Loss: 0.00041\n",
      "Epoch [7800/40000], Loss: 0.00040\n",
      "Epoch [7900/40000], Loss: 0.00040\n",
      "Epoch [8000/40000], Loss: 0.00040\n",
      "Epoch [8100/40000], Loss: 0.00039\n",
      "Epoch [8200/40000], Loss: 0.00039\n",
      "Epoch [8300/40000], Loss: 0.00039\n",
      "Epoch [8400/40000], Loss: 0.00038\n",
      "Epoch [8500/40000], Loss: 0.00038\n",
      "Epoch [8600/40000], Loss: 0.00038\n",
      "Epoch [8700/40000], Loss: 0.00037\n",
      "Epoch [8800/40000], Loss: 0.00037\n",
      "Epoch [8900/40000], Loss: 0.00037\n",
      "Epoch [9000/40000], Loss: 0.00037\n",
      "Epoch [9100/40000], Loss: 0.00036\n",
      "Epoch [9200/40000], Loss: 0.00036\n",
      "Epoch [9300/40000], Loss: 0.00036\n",
      "Epoch [9400/40000], Loss: 0.00035\n",
      "Epoch [9500/40000], Loss: 0.00035\n",
      "Epoch [9600/40000], Loss: 0.00035\n",
      "Epoch [9700/40000], Loss: 0.00034\n",
      "Epoch [9800/40000], Loss: 0.00034\n",
      "Epoch [9900/40000], Loss: 0.00034\n",
      "Epoch [10000/40000], Loss: 0.00034\n",
      "Epoch [10100/40000], Loss: 0.00033\n",
      "Epoch [10200/40000], Loss: 0.00033\n",
      "Epoch [10300/40000], Loss: 0.00033\n",
      "Epoch [10400/40000], Loss: 0.00032\n",
      "Epoch [10500/40000], Loss: 0.00032\n",
      "Epoch [10600/40000], Loss: 0.00032\n",
      "Epoch [10700/40000], Loss: 0.00032\n",
      "Epoch [10800/40000], Loss: 0.00031\n",
      "Epoch [10900/40000], Loss: 0.00031\n",
      "Epoch [11000/40000], Loss: 0.00031\n",
      "Epoch [11100/40000], Loss: 0.00031\n",
      "Epoch [11200/40000], Loss: 0.00030\n",
      "Epoch [11300/40000], Loss: 0.00030\n",
      "Epoch [11400/40000], Loss: 0.00030\n",
      "Epoch [11500/40000], Loss: 0.00030\n",
      "Epoch [11600/40000], Loss: 0.00029\n",
      "Epoch [11700/40000], Loss: 0.00029\n",
      "Epoch [11800/40000], Loss: 0.00029\n",
      "Epoch [11900/40000], Loss: 0.00029\n",
      "Epoch [12000/40000], Loss: 0.00028\n",
      "Epoch [12100/40000], Loss: 0.00028\n",
      "Epoch [12200/40000], Loss: 0.00028\n",
      "Epoch [12300/40000], Loss: 0.00028\n",
      "Epoch [12400/40000], Loss: 0.00027\n",
      "Epoch [12500/40000], Loss: 0.00027\n",
      "Epoch [12600/40000], Loss: 0.00027\n",
      "Epoch [12700/40000], Loss: 0.00027\n",
      "Epoch [12800/40000], Loss: 0.00026\n",
      "Epoch [12900/40000], Loss: 0.00026\n",
      "Epoch [13000/40000], Loss: 0.00026\n",
      "Epoch [13100/40000], Loss: 0.00026\n",
      "Epoch [13200/40000], Loss: 0.00026\n",
      "Epoch [13300/40000], Loss: 0.00025\n",
      "Epoch [13400/40000], Loss: 0.00025\n",
      "Epoch [13500/40000], Loss: 0.00025\n",
      "Epoch [13600/40000], Loss: 0.00025\n",
      "Epoch [13700/40000], Loss: 0.00025\n",
      "Epoch [13800/40000], Loss: 0.00024\n",
      "Epoch [13900/40000], Loss: 0.00024\n",
      "Epoch [14000/40000], Loss: 0.00024\n",
      "Epoch [14100/40000], Loss: 0.00024\n",
      "Epoch [14200/40000], Loss: 0.00024\n",
      "Epoch [14300/40000], Loss: 0.00023\n",
      "Epoch [14400/40000], Loss: 0.00023\n",
      "Epoch [14500/40000], Loss: 0.00023\n",
      "Epoch [14600/40000], Loss: 0.00023\n",
      "Epoch [14700/40000], Loss: 0.00023\n",
      "Epoch [14800/40000], Loss: 0.00022\n",
      "Epoch [14900/40000], Loss: 0.00022\n",
      "Epoch [15000/40000], Loss: 0.00022\n",
      "Epoch [15100/40000], Loss: 0.00022\n",
      "Epoch [15200/40000], Loss: 0.00022\n",
      "Epoch [15300/40000], Loss: 0.00021\n",
      "Epoch [15400/40000], Loss: 0.00021\n",
      "Epoch [15500/40000], Loss: 0.00021\n",
      "Epoch [15600/40000], Loss: 0.00021\n",
      "Epoch [15700/40000], Loss: 0.00021\n",
      "Epoch [15800/40000], Loss: 0.00021\n",
      "Epoch [15900/40000], Loss: 0.00020\n",
      "Epoch [16000/40000], Loss: 0.00020\n",
      "Epoch [16100/40000], Loss: 0.00020\n",
      "Epoch [16200/40000], Loss: 0.00020\n",
      "Epoch [16300/40000], Loss: 0.00020\n",
      "Epoch [16400/40000], Loss: 0.00020\n",
      "Epoch [16500/40000], Loss: 0.00019\n",
      "Epoch [16600/40000], Loss: 0.00019\n",
      "Epoch [16700/40000], Loss: 0.00019\n",
      "Epoch [16800/40000], Loss: 0.00019\n",
      "Epoch [16900/40000], Loss: 0.00019\n",
      "Epoch [17000/40000], Loss: 0.00019\n",
      "Epoch [17100/40000], Loss: 0.00018\n",
      "Epoch [17200/40000], Loss: 0.00018\n",
      "Epoch [17300/40000], Loss: 0.00018\n",
      "Epoch [17400/40000], Loss: 0.00018\n",
      "Epoch [17500/40000], Loss: 0.00018\n",
      "Epoch [17600/40000], Loss: 0.00018\n",
      "Epoch [17700/40000], Loss: 0.00018\n",
      "Epoch [17800/40000], Loss: 0.00017\n",
      "Epoch [17900/40000], Loss: 0.00017\n",
      "Epoch [18000/40000], Loss: 0.00017\n",
      "Epoch [18100/40000], Loss: 0.00017\n",
      "Epoch [18200/40000], Loss: 0.00017\n",
      "Epoch [18300/40000], Loss: 0.00017\n",
      "Epoch [18400/40000], Loss: 0.00017\n",
      "Epoch [18500/40000], Loss: 0.00016\n",
      "Epoch [18600/40000], Loss: 0.00016\n",
      "Epoch [18700/40000], Loss: 0.00016\n",
      "Epoch [18800/40000], Loss: 0.00016\n",
      "Epoch [18900/40000], Loss: 0.00016\n",
      "Epoch [19000/40000], Loss: 0.00016\n",
      "Epoch [19100/40000], Loss: 0.00016\n",
      "Epoch [19200/40000], Loss: 0.00015\n",
      "Epoch [19300/40000], Loss: 0.00015\n",
      "Epoch [19400/40000], Loss: 0.00015\n",
      "Epoch [19500/40000], Loss: 0.00015\n",
      "Epoch [19600/40000], Loss: 0.00015\n",
      "Epoch [19700/40000], Loss: 0.00015\n",
      "Epoch [19800/40000], Loss: 0.00015\n",
      "Epoch [19900/40000], Loss: 0.00015\n",
      "Epoch [20000/40000], Loss: 0.00014\n",
      "Epoch [20100/40000], Loss: 0.00014\n",
      "Epoch [20200/40000], Loss: 0.00014\n",
      "Epoch [20300/40000], Loss: 0.00014\n",
      "Epoch [20400/40000], Loss: 0.00014\n",
      "Epoch [20500/40000], Loss: 0.00014\n",
      "Epoch [20600/40000], Loss: 0.00014\n",
      "Epoch [20700/40000], Loss: 0.00014\n",
      "Epoch [20800/40000], Loss: 0.00014\n",
      "Epoch [20900/40000], Loss: 0.00013\n",
      "Epoch [21000/40000], Loss: 0.00013\n",
      "Epoch [21100/40000], Loss: 0.00013\n",
      "Epoch [21200/40000], Loss: 0.00013\n",
      "Epoch [21300/40000], Loss: 0.00013\n",
      "Epoch [21400/40000], Loss: 0.00013\n",
      "Epoch [21500/40000], Loss: 0.00013\n",
      "Epoch [21600/40000], Loss: 0.00013\n",
      "Epoch [21700/40000], Loss: 0.00013\n",
      "Epoch [21800/40000], Loss: 0.00012\n",
      "Epoch [21900/40000], Loss: 0.00012\n",
      "Epoch [22000/40000], Loss: 0.00012\n",
      "Epoch [22100/40000], Loss: 0.00012\n",
      "Epoch [22200/40000], Loss: 0.00012\n",
      "Epoch [22300/40000], Loss: 0.00012\n",
      "Epoch [22400/40000], Loss: 0.00012\n",
      "Epoch [22500/40000], Loss: 0.00012\n",
      "Epoch [22600/40000], Loss: 0.00012\n",
      "Epoch [22700/40000], Loss: 0.00012\n",
      "Epoch [22800/40000], Loss: 0.00011\n",
      "Epoch [22900/40000], Loss: 0.00011\n",
      "Epoch [23000/40000], Loss: 0.00011\n",
      "Epoch [23100/40000], Loss: 0.00011\n",
      "Epoch [23200/40000], Loss: 0.00011\n",
      "Epoch [23300/40000], Loss: 0.00011\n",
      "Epoch [23400/40000], Loss: 0.00011\n",
      "Epoch [23500/40000], Loss: 0.00011\n",
      "Epoch [23600/40000], Loss: 0.00011\n",
      "Epoch [23700/40000], Loss: 0.00011\n",
      "Epoch [23800/40000], Loss: 0.00011\n",
      "Epoch [23900/40000], Loss: 0.00010\n",
      "Epoch [24000/40000], Loss: 0.00010\n",
      "Epoch [24100/40000], Loss: 0.00010\n",
      "Epoch [24200/40000], Loss: 0.00010\n",
      "Epoch [24300/40000], Loss: 0.00010\n",
      "Epoch [24400/40000], Loss: 0.00010\n",
      "Epoch [24500/40000], Loss: 0.00010\n",
      "Epoch [24600/40000], Loss: 0.00010\n",
      "Epoch [24700/40000], Loss: 0.00010\n",
      "Epoch [24800/40000], Loss: 0.00010\n",
      "Epoch [24900/40000], Loss: 0.00010\n",
      "Epoch [25000/40000], Loss: 0.00010\n",
      "Epoch [25100/40000], Loss: 0.00009\n",
      "Epoch [25200/40000], Loss: 0.00009\n",
      "Epoch [25300/40000], Loss: 0.00009\n",
      "Epoch [25400/40000], Loss: 0.00009\n",
      "Epoch [25500/40000], Loss: 0.00009\n",
      "Epoch [25600/40000], Loss: 0.00009\n",
      "Epoch [25700/40000], Loss: 0.00009\n",
      "Epoch [25800/40000], Loss: 0.00009\n",
      "Epoch [25900/40000], Loss: 0.00009\n",
      "Epoch [26000/40000], Loss: 0.00009\n",
      "Epoch [26100/40000], Loss: 0.00009\n",
      "Epoch [26200/40000], Loss: 0.00009\n",
      "Epoch [26300/40000], Loss: 0.00009\n",
      "Epoch [26400/40000], Loss: 0.00008\n",
      "Epoch [26500/40000], Loss: 0.00008\n",
      "Epoch [26600/40000], Loss: 0.00008\n",
      "Epoch [26700/40000], Loss: 0.00008\n",
      "Epoch [26800/40000], Loss: 0.00008\n",
      "Epoch [26900/40000], Loss: 0.00008\n",
      "Epoch [27000/40000], Loss: 0.00008\n",
      "Epoch [27100/40000], Loss: 0.00008\n",
      "Epoch [27200/40000], Loss: 0.00008\n",
      "Epoch [27300/40000], Loss: 0.00008\n",
      "Epoch [27400/40000], Loss: 0.00008\n",
      "Epoch [27500/40000], Loss: 0.00008\n",
      "Epoch [27600/40000], Loss: 0.00008\n",
      "Epoch [27700/40000], Loss: 0.00008\n",
      "Epoch [27800/40000], Loss: 0.00008\n",
      "Epoch [27900/40000], Loss: 0.00008\n",
      "Epoch [28000/40000], Loss: 0.00007\n",
      "Epoch [28100/40000], Loss: 0.00007\n",
      "Epoch [28200/40000], Loss: 0.00007\n",
      "Epoch [28300/40000], Loss: 0.00007\n",
      "Epoch [28400/40000], Loss: 0.00007\n",
      "Epoch [28500/40000], Loss: 0.00007\n",
      "Epoch [28600/40000], Loss: 0.00007\n",
      "Epoch [28700/40000], Loss: 0.00007\n",
      "Epoch [28800/40000], Loss: 0.00007\n",
      "Epoch [28900/40000], Loss: 0.00007\n",
      "Epoch [29000/40000], Loss: 0.00007\n",
      "Epoch [29100/40000], Loss: 0.00007\n",
      "Epoch [29200/40000], Loss: 0.00007\n",
      "Epoch [29300/40000], Loss: 0.00007\n",
      "Epoch [29400/40000], Loss: 0.00007\n",
      "Epoch [29500/40000], Loss: 0.00007\n",
      "Epoch [29600/40000], Loss: 0.00007\n",
      "Epoch [29700/40000], Loss: 0.00006\n",
      "Epoch [29800/40000], Loss: 0.00006\n",
      "Epoch [29900/40000], Loss: 0.00006\n",
      "Epoch [30000/40000], Loss: 0.00006\n",
      "Epoch [30100/40000], Loss: 0.00006\n",
      "Epoch [30200/40000], Loss: 0.00006\n",
      "Epoch [30300/40000], Loss: 0.00006\n",
      "Epoch [30400/40000], Loss: 0.00006\n",
      "Epoch [30500/40000], Loss: 0.00006\n",
      "Epoch [30600/40000], Loss: 0.00006\n",
      "Epoch [30700/40000], Loss: 0.00006\n",
      "Epoch [30800/40000], Loss: 0.00006\n",
      "Epoch [30900/40000], Loss: 0.00006\n",
      "Epoch [31000/40000], Loss: 0.00006\n",
      "Epoch [31100/40000], Loss: 0.00006\n",
      "Epoch [31200/40000], Loss: 0.00006\n",
      "Epoch [31300/40000], Loss: 0.00006\n",
      "Epoch [31400/40000], Loss: 0.00006\n",
      "Epoch [31500/40000], Loss: 0.00006\n",
      "Epoch [31600/40000], Loss: 0.00006\n",
      "Epoch [31700/40000], Loss: 0.00005\n",
      "Epoch [31800/40000], Loss: 0.00005\n",
      "Epoch [31900/40000], Loss: 0.00005\n",
      "Epoch [32000/40000], Loss: 0.00005\n",
      "Epoch [32100/40000], Loss: 0.00005\n",
      "Epoch [32200/40000], Loss: 0.00005\n",
      "Epoch [32300/40000], Loss: 0.00005\n",
      "Epoch [32400/40000], Loss: 0.00005\n",
      "Epoch [32500/40000], Loss: 0.00005\n",
      "Epoch [32600/40000], Loss: 0.00005\n",
      "Epoch [32700/40000], Loss: 0.00005\n",
      "Epoch [32800/40000], Loss: 0.00005\n",
      "Epoch [32900/40000], Loss: 0.00005\n",
      "Epoch [33000/40000], Loss: 0.00005\n",
      "Epoch [33100/40000], Loss: 0.00005\n",
      "Epoch [33200/40000], Loss: 0.00005\n",
      "Epoch [33300/40000], Loss: 0.00005\n",
      "Epoch [33400/40000], Loss: 0.00005\n",
      "Epoch [33500/40000], Loss: 0.00005\n",
      "Epoch [33600/40000], Loss: 0.00005\n",
      "Epoch [33700/40000], Loss: 0.00005\n",
      "Epoch [33800/40000], Loss: 0.00005\n",
      "Epoch [33900/40000], Loss: 0.00005\n",
      "Epoch [34000/40000], Loss: 0.00005\n",
      "Epoch [34100/40000], Loss: 0.00004\n",
      "Epoch [34200/40000], Loss: 0.00004\n",
      "Epoch [34300/40000], Loss: 0.00004\n",
      "Epoch [34400/40000], Loss: 0.00004\n",
      "Epoch [34500/40000], Loss: 0.00004\n",
      "Epoch [34600/40000], Loss: 0.00004\n",
      "Epoch [34700/40000], Loss: 0.00004\n",
      "Epoch [34800/40000], Loss: 0.00004\n",
      "Epoch [34900/40000], Loss: 0.00004\n",
      "Epoch [35000/40000], Loss: 0.00004\n",
      "Epoch [35100/40000], Loss: 0.00004\n",
      "Epoch [35200/40000], Loss: 0.00004\n",
      "Epoch [35300/40000], Loss: 0.00004\n",
      "Epoch [35400/40000], Loss: 0.00004\n",
      "Epoch [35500/40000], Loss: 0.00004\n",
      "Epoch [35600/40000], Loss: 0.00004\n",
      "Epoch [35700/40000], Loss: 0.00004\n",
      "Epoch [35800/40000], Loss: 0.00004\n",
      "Epoch [35900/40000], Loss: 0.00004\n",
      "Epoch [36000/40000], Loss: 0.00004\n",
      "Epoch [36100/40000], Loss: 0.00004\n",
      "Epoch [36200/40000], Loss: 0.00004\n",
      "Epoch [36300/40000], Loss: 0.00004\n",
      "Epoch [36400/40000], Loss: 0.00004\n",
      "Epoch [36500/40000], Loss: 0.00004\n",
      "Epoch [36600/40000], Loss: 0.00004\n",
      "Epoch [36700/40000], Loss: 0.00004\n",
      "Epoch [36800/40000], Loss: 0.00004\n",
      "Epoch [36900/40000], Loss: 0.00004\n",
      "Epoch [37000/40000], Loss: 0.00004\n",
      "Epoch [37100/40000], Loss: 0.00004\n",
      "Epoch [37200/40000], Loss: 0.00003\n",
      "Epoch [37300/40000], Loss: 0.00003\n",
      "Epoch [37400/40000], Loss: 0.00003\n",
      "Epoch [37500/40000], Loss: 0.00003\n",
      "Epoch [37600/40000], Loss: 0.00003\n",
      "Epoch [37700/40000], Loss: 0.00003\n",
      "Epoch [37800/40000], Loss: 0.00003\n",
      "Epoch [37900/40000], Loss: 0.00003\n",
      "Epoch [38000/40000], Loss: 0.00003\n",
      "Epoch [38100/40000], Loss: 0.00003\n",
      "Epoch [38200/40000], Loss: 0.00003\n",
      "Epoch [38300/40000], Loss: 0.00003\n",
      "Epoch [38400/40000], Loss: 0.00003\n",
      "Epoch [38500/40000], Loss: 0.00003\n",
      "Epoch [38600/40000], Loss: 0.00003\n",
      "Epoch [38700/40000], Loss: 0.00003\n",
      "Epoch [38800/40000], Loss: 0.00003\n",
      "Epoch [38900/40000], Loss: 0.00003\n",
      "Epoch [39000/40000], Loss: 0.00003\n",
      "Epoch [39100/40000], Loss: 0.00003\n",
      "Epoch [39200/40000], Loss: 0.00003\n",
      "Epoch [39300/40000], Loss: 0.00003\n",
      "Epoch [39400/40000], Loss: 0.00003\n",
      "Epoch [39500/40000], Loss: 0.00003\n",
      "Epoch [39600/40000], Loss: 0.00003\n",
      "Epoch [39700/40000], Loss: 0.00003\n",
      "Epoch [39800/40000], Loss: 0.00003\n",
      "Epoch [39900/40000], Loss: 0.00003\n",
      "\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# train the student\n",
    "student_model, losses = train(\n",
    "    model = student_model, \n",
    "    x_train = data[0], \n",
    "    y_train= data[1], \n",
    "    num_epochs = 40000, \n",
    "    lr = 1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3600945  -1.1257662  -0.39243376 -0.816296  ]\n",
      " [-1.1257662   2.1519432  -0.49748465  0.74801093]\n",
      " [-0.39243376 -0.49748465  2.2769542   0.6069933 ]\n",
      " [-0.816296    0.74801093  0.6069933   1.3959402 ]]\n",
      "[[ 3.3376353  -1.0867145  -0.35086843 -0.8388499 ]\n",
      " [-1.0867145   2.1277146  -0.5053588   0.76081246]\n",
      " [-0.35086843 -0.5053588   2.3117304   0.61279106]\n",
      " [-0.8388499   0.76081246  0.61279106  1.3862303 ]]\n",
      "[4.4600277 1.4670224 0.5730873 2.6847944]\n",
      "[4.418686  1.4791214 0.5549545 2.7105486]\n",
      "Population loss = 0.021259774\n"
     ]
    }
   ],
   "source": [
    "# lets look at the weight matrices\n",
    "# print(student_model.layers[0].weight)\n",
    "student_w = student_model.layers[0].weight.detach().numpy()\n",
    "# student_v = student_model.layers[2].weight.detach().numpy()\n",
    "# print(student_w)\n",
    "teacher_w = teacher_model.layers[0].weight.detach().numpy()\n",
    "# teacher_v = teacher_model.layers[2].weight.detach().numpy()\n",
    "# print(student_w)\n",
    "# print(teacher_w)\n",
    "# print(student_v)\n",
    "# print(teacher_v)\n",
    "# print(np.squeeze(student_v))\n",
    "# A = student_w.T @ np.diag(np.squeeze(student_v)) @ student_w\n",
    "# Astar = teacher_w.T @ np.diag(np.squeeze(teacher_v)) @ teacher_w\n",
    "A = student_w.T @ student_w\n",
    "Astar = teacher_w.T @ teacher_w\n",
    "print(A)\n",
    "print(Astar)\n",
    "eigA, eigvecA = np.linalg.eig(A)\n",
    "print(eigA)\n",
    "# print(Astar)\n",
    "eigAstar, eigvecAstar = np.linalg.eig(Astar)\n",
    "print(eigAstar)\n",
    "e = pop_loss(student_model, teacher_model, d=d, N=10000)\n",
    "# print(y_teach)\n",
    "# print(y_stud)\n",
    "# print(torch.norm(y_teach))\n",
    "print(\"Population loss = \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m]])\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;129;43m@A\u001b[39;49m\u001b[38;5;129m@test_vec\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(test_vec.T@Astar@test_vec)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m test_vec_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(test_vec\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 2)"
     ]
    }
   ],
   "source": [
    "test_vec = np.array([[1,2]]).T\n",
    "print(test_vec.T@A@test_vec)\n",
    "# print(test_vec.T@Astar@test_vec)\n",
    "test_vec_torch = torch.from_numpy(test_vec.T).type(torch.FloatTensor)\n",
    "print(test_vec_torch)\n",
    "print(student_model(test_vec_torch))\n",
    "print(student_model.layers[2].weight)\n",
    "print(teacher_model.layers[2].weight)\n",
    "# print(teacher_model(test_vec_torch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

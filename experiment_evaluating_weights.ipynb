{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monomial_neural_network import *\n",
    "from experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonomialNeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=4, bias=False)\n",
      "    (1): Monomial()\n",
      "    (2): Linear(in_features=4, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "tensor([1.4225, 0.9157]) tensor([6.2432]) tensor([6.2432])\n",
      "MonomialNeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=4, bias=False)\n",
      "    (1): Monomial()\n",
      "    (2): Linear(in_features=4, out_features=1, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "d = 2 # input data dimension\n",
    "teacher_k = [4] # teacher model hidden layer sizes - 3 layers with increasing number of neurons\n",
    "# teacher_k = [10] # teacher model hidden layer sizes - 1 layer with 10 neurons\n",
    "\n",
    "teacher_model = generate_teacher_model(d, teacher_k)\n",
    "print(teacher_model)\n",
    "\n",
    "n = 1000 # number of data points\n",
    "# same data dimension d as before\n",
    "\n",
    "data = generate_data(n, d, teacher_model)\n",
    "\n",
    "# verify that the data is generated correctly\n",
    "print(data[0][0], data[1][0], teacher_model.evaluate(data[0][0]))\n",
    "\n",
    "student_k = [4] # student model hidden layer sizes - 2 layers with increasing number of neurons\n",
    "student_model = generate_student_model(d, student_k)\n",
    "print(student_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Epoch [0/20000], Loss: 31.72252\n",
      "Epoch [100/20000], Loss: 27.09764\n",
      "Epoch [200/20000], Loss: 22.83154\n",
      "Epoch [300/20000], Loss: 18.97034\n",
      "Epoch [400/20000], Loss: 15.59056\n",
      "Epoch [500/20000], Loss: 12.77433\n",
      "Epoch [600/20000], Loss: 10.55288\n",
      "Epoch [700/20000], Loss: 8.80693\n",
      "Epoch [800/20000], Loss: 7.28398\n",
      "Epoch [900/20000], Loss: 5.82847\n",
      "Epoch [1000/20000], Loss: 4.42367\n",
      "Epoch [1100/20000], Loss: 3.10599\n",
      "Epoch [1200/20000], Loss: 1.93103\n",
      "Epoch [1300/20000], Loss: 0.96729\n",
      "Epoch [1400/20000], Loss: 0.29509\n",
      "Epoch [1500/20000], Loss: 0.00539\n",
      "Epoch [1600/20000], Loss: 0.00000\n",
      "Epoch [1700/20000], Loss: 0.00000\n",
      "Epoch [1800/20000], Loss: 0.00000\n",
      "Epoch [1900/20000], Loss: 0.00000\n",
      "Epoch [2000/20000], Loss: 0.00000\n",
      "Epoch [2100/20000], Loss: 0.00000\n",
      "Epoch [2200/20000], Loss: 0.00000\n",
      "Epoch [2300/20000], Loss: 0.00000\n",
      "Epoch [2400/20000], Loss: 0.00000\n",
      "Epoch [2500/20000], Loss: 0.00000\n",
      "Epoch [2600/20000], Loss: 0.00000\n",
      "Epoch [2700/20000], Loss: 0.00000\n",
      "Epoch [2800/20000], Loss: 0.00000\n",
      "Epoch [2900/20000], Loss: 0.00000\n",
      "Epoch [3000/20000], Loss: 0.00000\n",
      "Epoch [3100/20000], Loss: 0.00000\n",
      "Epoch [3200/20000], Loss: 0.00000\n",
      "Epoch [3300/20000], Loss: 0.00000\n",
      "Epoch [3400/20000], Loss: 0.00000\n",
      "Epoch [3500/20000], Loss: 0.00000\n",
      "Epoch [3600/20000], Loss: 0.00000\n",
      "Epoch [3700/20000], Loss: 0.00000\n",
      "Epoch [3800/20000], Loss: 0.00000\n",
      "Epoch [3900/20000], Loss: 0.00000\n",
      "Epoch [4000/20000], Loss: 0.00000\n",
      "Epoch [4100/20000], Loss: 0.00000\n",
      "Epoch [4200/20000], Loss: 0.00000\n",
      "Epoch [4300/20000], Loss: 0.00000\n",
      "Epoch [4400/20000], Loss: 0.00000\n",
      "Epoch [4500/20000], Loss: 0.00000\n",
      "Epoch [4600/20000], Loss: 0.00000\n",
      "Epoch [4700/20000], Loss: 0.00000\n",
      "Epoch [4800/20000], Loss: 0.00000\n",
      "Epoch [4900/20000], Loss: 0.00000\n",
      "Epoch [5000/20000], Loss: 0.00000\n",
      "Epoch [5100/20000], Loss: 0.00000\n",
      "Epoch [5200/20000], Loss: 0.00000\n",
      "Epoch [5300/20000], Loss: 0.00000\n",
      "Epoch [5400/20000], Loss: 0.00000\n",
      "Epoch [5500/20000], Loss: 0.00000\n",
      "Epoch [5600/20000], Loss: 0.00000\n",
      "Epoch [5700/20000], Loss: 0.00000\n",
      "Epoch [5800/20000], Loss: 0.00000\n",
      "Epoch [5900/20000], Loss: 0.00000\n",
      "Epoch [6000/20000], Loss: 0.00000\n",
      "Epoch [6100/20000], Loss: 0.00000\n",
      "Epoch [6200/20000], Loss: 0.00000\n",
      "Epoch [6300/20000], Loss: 0.00000\n",
      "Epoch [6400/20000], Loss: 0.00000\n",
      "Epoch [6500/20000], Loss: 0.00000\n",
      "Epoch [6600/20000], Loss: 0.00000\n",
      "Epoch [6700/20000], Loss: 0.00000\n",
      "Epoch [6800/20000], Loss: 0.00000\n",
      "Epoch [6900/20000], Loss: 0.00000\n",
      "Epoch [7000/20000], Loss: 0.00000\n",
      "Epoch [7100/20000], Loss: 0.00000\n",
      "Epoch [7200/20000], Loss: 0.00000\n",
      "Epoch [7300/20000], Loss: 0.00000\n",
      "Epoch [7400/20000], Loss: 0.00000\n",
      "Epoch [7500/20000], Loss: 0.00000\n",
      "Epoch [7600/20000], Loss: 0.00000\n",
      "Epoch [7700/20000], Loss: 0.00000\n",
      "Epoch [7800/20000], Loss: 0.00000\n",
      "Epoch [7900/20000], Loss: 0.00000\n",
      "Epoch [8000/20000], Loss: 0.00000\n",
      "Epoch [8100/20000], Loss: 0.00000\n",
      "Epoch [8200/20000], Loss: 0.00000\n",
      "Epoch [8300/20000], Loss: 0.00000\n",
      "Epoch [8400/20000], Loss: 0.00000\n",
      "Epoch [8500/20000], Loss: 0.00000\n",
      "Epoch [8600/20000], Loss: 0.00000\n",
      "Epoch [8700/20000], Loss: 0.00000\n",
      "Epoch [8800/20000], Loss: 0.00000\n",
      "Epoch [8900/20000], Loss: 0.00000\n",
      "Epoch [9000/20000], Loss: 0.00000\n",
      "Epoch [9100/20000], Loss: 0.00000\n",
      "Epoch [9200/20000], Loss: 0.00000\n",
      "Epoch [9300/20000], Loss: 0.00000\n",
      "Epoch [9400/20000], Loss: 0.00000\n",
      "Epoch [9500/20000], Loss: 0.00000\n",
      "Epoch [9600/20000], Loss: 0.00000\n",
      "Epoch [9700/20000], Loss: 0.00000\n",
      "Epoch [9800/20000], Loss: 0.00000\n",
      "Epoch [9900/20000], Loss: 0.00000\n",
      "Epoch [10000/20000], Loss: 0.00000\n",
      "Epoch [10100/20000], Loss: 0.00000\n",
      "Epoch [10200/20000], Loss: 0.00000\n",
      "Epoch [10300/20000], Loss: 0.00000\n",
      "Epoch [10400/20000], Loss: 0.00000\n",
      "Epoch [10500/20000], Loss: 0.00000\n",
      "Epoch [10600/20000], Loss: 0.00000\n",
      "Epoch [10700/20000], Loss: 0.00000\n",
      "Epoch [10800/20000], Loss: 0.00000\n",
      "Epoch [10900/20000], Loss: 0.00000\n",
      "Epoch [11000/20000], Loss: 0.00000\n",
      "Epoch [11100/20000], Loss: 0.00000\n",
      "Epoch [11200/20000], Loss: 0.00000\n",
      "Epoch [11300/20000], Loss: 0.00000\n",
      "Epoch [11400/20000], Loss: 0.00000\n",
      "Epoch [11500/20000], Loss: 0.00000\n",
      "Epoch [11600/20000], Loss: 0.00000\n",
      "Epoch [11700/20000], Loss: 0.00000\n",
      "Epoch [11800/20000], Loss: 0.00000\n",
      "Epoch [11900/20000], Loss: 0.00000\n",
      "Epoch [12000/20000], Loss: 0.00000\n",
      "Epoch [12100/20000], Loss: 0.00000\n",
      "Epoch [12200/20000], Loss: 0.00000\n",
      "Epoch [12300/20000], Loss: 0.00000\n",
      "Epoch [12400/20000], Loss: 0.00000\n",
      "Epoch [12500/20000], Loss: 0.00000\n",
      "Epoch [12600/20000], Loss: 0.00000\n",
      "Epoch [12700/20000], Loss: 0.00000\n",
      "Epoch [12800/20000], Loss: 0.00000\n",
      "Epoch [12900/20000], Loss: 0.00000\n",
      "Epoch [13000/20000], Loss: 0.00000\n",
      "Epoch [13100/20000], Loss: 0.00000\n",
      "Epoch [13200/20000], Loss: 0.00000\n",
      "Epoch [13300/20000], Loss: 0.00000\n",
      "Epoch [13400/20000], Loss: 0.00000\n",
      "Epoch [13500/20000], Loss: 0.00000\n",
      "Epoch [13600/20000], Loss: 0.00000\n",
      "Epoch [13700/20000], Loss: 0.00000\n",
      "Epoch [13800/20000], Loss: 0.00000\n",
      "Epoch [13900/20000], Loss: 0.00000\n",
      "Epoch [14000/20000], Loss: 0.00000\n",
      "Epoch [14100/20000], Loss: 0.00000\n",
      "Epoch [14200/20000], Loss: 0.00000\n",
      "Epoch [14300/20000], Loss: 0.00000\n",
      "Epoch [14400/20000], Loss: 0.00000\n",
      "Epoch [14500/20000], Loss: 0.00000\n",
      "Epoch [14600/20000], Loss: 0.00000\n",
      "Epoch [14700/20000], Loss: 0.00000\n",
      "Epoch [14800/20000], Loss: 0.00000\n",
      "Epoch [14900/20000], Loss: 0.00000\n",
      "Epoch [15000/20000], Loss: 0.00000\n",
      "Epoch [15100/20000], Loss: 0.00000\n",
      "Epoch [15200/20000], Loss: 0.00000\n",
      "Epoch [15300/20000], Loss: 0.00000\n",
      "Epoch [15400/20000], Loss: 0.00000\n",
      "Epoch [15500/20000], Loss: 0.00000\n",
      "Epoch [15600/20000], Loss: 0.00000\n",
      "Epoch [15700/20000], Loss: 0.00000\n",
      "Epoch [15800/20000], Loss: 0.00000\n",
      "Epoch [15900/20000], Loss: 0.00000\n",
      "Epoch [16000/20000], Loss: 0.00000\n",
      "Epoch [16100/20000], Loss: 0.00000\n",
      "Epoch [16200/20000], Loss: 0.00000\n",
      "Epoch [16300/20000], Loss: 0.00000\n",
      "Epoch [16400/20000], Loss: 0.00000\n",
      "Epoch [16500/20000], Loss: 0.00000\n",
      "Epoch [16600/20000], Loss: 0.00000\n",
      "Epoch [16700/20000], Loss: 0.00000\n",
      "Epoch [16800/20000], Loss: 0.00000\n",
      "Epoch [16900/20000], Loss: 0.00000\n",
      "Epoch [17000/20000], Loss: 0.00000\n",
      "Epoch [17100/20000], Loss: 0.00000\n",
      "Epoch [17200/20000], Loss: 0.00000\n",
      "Epoch [17300/20000], Loss: 0.00000\n",
      "Epoch [17400/20000], Loss: 0.00000\n",
      "Epoch [17500/20000], Loss: 0.00000\n",
      "Epoch [17600/20000], Loss: 0.00000\n",
      "Epoch [17700/20000], Loss: 0.00000\n",
      "Epoch [17800/20000], Loss: 0.00000\n",
      "Epoch [17900/20000], Loss: 0.00000\n",
      "Epoch [18000/20000], Loss: 0.00000\n",
      "Epoch [18100/20000], Loss: 0.00000\n",
      "Epoch [18200/20000], Loss: 0.00000\n",
      "Epoch [18300/20000], Loss: 0.00000\n",
      "Epoch [18400/20000], Loss: 0.00000\n",
      "Epoch [18500/20000], Loss: 0.00000\n",
      "Epoch [18600/20000], Loss: 0.00000\n",
      "Epoch [18700/20000], Loss: 0.00000\n",
      "Epoch [18800/20000], Loss: 0.00000\n",
      "Epoch [18900/20000], Loss: 0.00000\n",
      "Epoch [19000/20000], Loss: 0.00000\n",
      "Epoch [19100/20000], Loss: 0.00000\n",
      "Epoch [19200/20000], Loss: 0.00000\n",
      "Epoch [19300/20000], Loss: 0.00000\n",
      "Epoch [19400/20000], Loss: 0.00000\n",
      "Epoch [19500/20000], Loss: 0.00000\n",
      "Epoch [19600/20000], Loss: 0.00000\n",
      "Epoch [19700/20000], Loss: 0.00000\n",
      "Epoch [19800/20000], Loss: 0.00000\n",
      "Epoch [19900/20000], Loss: 0.00000\n",
      "\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# train the student\n",
    "student_model, losses = train(\n",
    "    model = student_model, \n",
    "    x_train = data[0], \n",
    "    y_train= data[1], \n",
    "    num_epochs = 20000, \n",
    "    lr = 1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.9341867  -0.50317925]\n",
      " [-0.5031793   1.9277991 ]]\n",
      "[[ 2.9341826 -0.5031786]\n",
      " [-0.5031786  1.9278024]]\n",
      "[3.142606  1.7193797]\n",
      "[3.1426027 1.7193824]\n",
      "Population loss = 7.094969e-10\n"
     ]
    }
   ],
   "source": [
    "# lets look at the weight matrices\n",
    "# print(student_model.layers[0].weight)\n",
    "student_w = student_model.layers[0].weight.detach().numpy()\n",
    "student_v = student_model.layers[2].weight.detach().numpy()\n",
    "# print(student_w)\n",
    "teacher_w = teacher_model.layers[0].weight.detach().numpy()\n",
    "teacher_v = teacher_model.layers[2].weight.detach().numpy()\n",
    "# print(student_w)\n",
    "# print(teacher_w)\n",
    "# print(np.squeeze(student_v))\n",
    "A = student_w.T @ np.diag(np.squeeze(student_v)) @ student_w\n",
    "Astar = teacher_w.T @ np.diag(np.squeeze(teacher_v)) @ teacher_w\n",
    "print(A)\n",
    "print(Astar)\n",
    "eigA, eigvecA = np.linalg.eig(A)\n",
    "print(eigA)\n",
    "# print(Astar)\n",
    "eigAstar, eigvecAstar = np.linalg.eig(Astar)\n",
    "print(eigAstar)\n",
    "e = pop_loss(student_model, teacher_model, d=d, N=100000)\n",
    "# print(y_teach)\n",
    "# print(y_stud)\n",
    "# print(torch.norm(y_teach))\n",
    "print(\"Population loss = \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.63266599]]\n",
      "tensor([[1., 2.]])\n",
      "tensor([[8.6327]], grad_fn=<MmBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 1.2476,  1.2093,  1.1667, -0.9397]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_vec = np.array([[1,2]]).T\n",
    "print(test_vec.T@A@test_vec)\n",
    "# print(test_vec.T@Astar@test_vec)\n",
    "test_vec_torch = torch.from_numpy(test_vec.T).type(torch.FloatTensor)\n",
    "print(test_vec_torch)\n",
    "print(student_model(test_vec_torch))\n",
    "print(student_model.layers[2].weight)\n",
    "print(teacher_model.layers[2].weight)\n",
    "# print(teacher_model(test_vec_torch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
